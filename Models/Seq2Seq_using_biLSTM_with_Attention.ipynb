{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20111070_hind_eng_NMT_Seq2Seq_Model_Week3_attention.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuLbPWyaagf"
      },
      "source": [
        "# For running the notebook, create a directory with name: \"Colab_Notebooks\" in Google Drive\n",
        "# Then save the data files \"train.csv\" & \"hindistatements.csv\" in the Colab_Notebooks directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7UhFbZmRZlw"
      },
      "source": [
        "## Model: Sequence to Sequence With Attention (Encoder: Bidirectional LSTM), (Decoder: LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_caFAenHP-d"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoL9dmHeAMCD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUB12htcqU9W"
      },
      "source": [
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MA9X771Hp0d"
      },
      "source": [
        "## Installing indicnlp module for tokenizing hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOnqfy0PE5OD"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mL4oItNBvSs"
      },
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import  re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "# For tokenizing english sentence\n",
        "import spacy\n",
        "# For tokenizing hindi sentence  \n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU2g5gTVFhrd"
      },
      "source": [
        "## Setting up the device based on availabilty of GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adyBDgMY-mgm"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMSzH2oFtt4"
      },
      "source": [
        "## Load default spacy english-core-web model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcyNDKXDd4q"
      },
      "source": [
        "spacy_english = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RpOzneLGFxD"
      },
      "source": [
        "## Function that returns list of tokens in an english sentence using sacy tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1hVyxIABZl"
      },
      "source": [
        "def spacy_english_tokenizer(sentence):\n",
        "  tokenized_sent = spacy_english.tokenizer(sentence)\n",
        "  token_list = []\n",
        "  for token in tokenized_sent:\n",
        "    token_list.append(token.text)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00eMgbiTGE-I"
      },
      "source": [
        "## Function that returns list of tokens in a hindi sentence using indicnlp tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gspj8cvvFroQ"
      },
      "source": [
        "def indicnlp_hindi_tokenizer(sentence):\n",
        "  token_list = indic_tokenize.trivial_tokenize(sentence)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO9tggsrGVrG"
      },
      "source": [
        "## Testing english tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2iQlJKxyWA"
      },
      "source": [
        "sentence = 'hello world! Here i come'\n",
        "# spacy_english_tokenizer(sentence)\n",
        "indicnlp_hindi_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lLvDZkGbcR"
      },
      "source": [
        "## Testing hindi tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUHY4pjQyk5A"
      },
      "source": [
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा।'\n",
        "indicnlp_hindi_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgkv7pQ1d3zE"
      },
      "source": [
        "maximum_length = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutXBe6Lv2I3"
      },
      "source": [
        "## Function that cleans Train data and saves the clean train data to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM6mZvvSv1UO"
      },
      "source": [
        "def clean_train_data():\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  regx_printable = re.compile('[%s]' % re.escape(string.printable))\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','w') as clean_csv_file:\n",
        "    csv_writer = csv.writer(clean_csv_file)\n",
        "    csv_writer.writerow(['hindi','english'])\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/train.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[1]\n",
        "        english_sen = line[2]\n",
        "        if len(hindi_sen) == 0 or len(english_sen) ==0:\n",
        "          continue\n",
        "        if len(hindi_sen) > 1.5* len(english_sen) or len(english_sen) > 1.5* len(hindi_sen):\n",
        "          continue\n",
        "        \n",
        "        if len(english_sen.split())>maximum_length or len(hindi_sen.split())>maximum_length:\n",
        "          continue\n",
        "        english_sen = english_sen.strip()\n",
        "        english_sen = normalize('NFD',english_sen).encode('ascii', 'ignore')\n",
        "        english_sen = english_sen.decode('UTF-8')\n",
        "        temp_str_words = english_sen.split()\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "        #temp_str_words = [regx_printable.sub('', word) for word in temp_str_words]\n",
        "        temp_str_words = [word.lower() for word in temp_str_words]\n",
        "        temp_str_words = [word for word in temp_str_words if word.isalpha()]\n",
        "        \n",
        "        temp_str_words.insert(0,'sos'); temp_str_words.append('eos')\n",
        "        english_sen = ' '.join(map(str, temp_str_words))\n",
        "\n",
        "        hindi_sen = hindi_sen.strip()\n",
        "        hindi_sen.replace('♪', '')\n",
        "        hindi_sen = normalize('NFD',hindi_sen).encode('UTF-8', 'ignore')\n",
        "        hindi_sen = hindi_sen.decode('UTF-8')\n",
        "        temp_str_words = hindi_sen.split()\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "        # temp_str_words = [regx_printable.sub('', word) for word in temp_str_words]\n",
        "        temp_str_words = [word.lower() for word in temp_str_words]\n",
        "        temp_str_words = [word for word in temp_str_words if not (word==' ')]\n",
        "       \n",
        "        temp_str_words.insert(0,'sos'); temp_str_words.append('eos')\n",
        "        hindi_sen = ' '.join(map(str, temp_str_words))\n",
        "        csv_writer.writerow([hindi_sen,english_sen])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17msSSAyzaO"
      },
      "source": [
        "## Cleaning the Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZujINLkFyuv7"
      },
      "source": [
        "clean_train_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiBSEo5xzFBQ"
      },
      "source": [
        "## Function that cleans test data and saves it to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5oyL4EvzMpT"
      },
      "source": [
        "def clean_test_data():\n",
        "   table = str.maketrans('', '', string.punctuation)\n",
        "   with open('/content/drive/MyDrive/Colab_Notebooks/clean_test.csv','w') as clean_csv_file:\n",
        "    csv_writer = csv.writer(clean_csv_file)\n",
        "    csv_writer.writerow(['hindi'])\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/testhindistatements.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[2]\n",
        "        hindi_sen = hindi_sen.strip()\n",
        "        hindi_sen = normalize('NFD',hindi_sen).encode('UTF-8', 'ignore')\n",
        "        hindi_sen = hindi_sen.decode('UTF-8')\n",
        "        temp_str_words = hindi_sen.split()\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "        # temp_str_words = [regx_printable.sub('', word) for word in temp_str_words]\n",
        "        temp_str_words = [word.lower() for word in temp_str_words]\n",
        "        temp_str_words = [word for word in temp_str_words if word != ' ']\n",
        "        hindi_sen = ' '.join(map(str, temp_str_words))\n",
        "\n",
        "        csv_writer.writerow([hindi_sen])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdo_MMw-y_00"
      },
      "source": [
        "## Cleaning the Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA3Vy7n-y-6f"
      },
      "source": [
        "clean_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X_Is3Qp8Dun"
      },
      "source": [
        "hindi_tok_to_ind_dict = {'pad':0, 'sos':1, 'eos':2}\n",
        "hindi_ind_to_tok_dict = {0:'pad', 1:'sos', 2:'eos'}\n",
        "english_tok_to_ind_dict = {'pad':0, 'sos':1, 'eos':2}\n",
        "english_ind_to_tok_dict = {0:'pad', 1:'sos', 2:'eos'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGle4_k4eok"
      },
      "source": [
        "def generate_hindi_vocab():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      index_count = 3\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[0]\n",
        "        tokens = indicnlp_hindi_tokenizer(hindi_sen)\n",
        "        for token in tokens:\n",
        "          if token not in hindi_tok_to_ind_dict:\n",
        "            hindi_tok_to_ind_dict[token] = index_count\n",
        "            hindi_ind_to_tok_dict[index_count] = token\n",
        "            index_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW7fP2Ln7gd2"
      },
      "source": [
        "generate_hindi_vocab()\n",
        "print(len(hindi_ind_to_tok_dict))\n",
        "print(len(hindi_tok_to_ind_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0m93nJl8U_W"
      },
      "source": [
        "def generate_english_vocab():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      index_count = 3\n",
        "      for line in csv_reader:\n",
        "        english_sen = line[1]\n",
        "        tokens = indicnlp_hindi_tokenizer(english_sen)\n",
        "        for token in tokens:\n",
        "          if token not in english_tok_to_ind_dict:\n",
        "            english_tok_to_ind_dict[token] = index_count\n",
        "            english_ind_to_tok_dict[index_count] = token\n",
        "            index_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIjqN1BF9Xyb"
      },
      "source": [
        "generate_english_vocab()\n",
        "print(len(english_ind_to_tok_dict))\n",
        "print(len(english_tok_to_ind_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-xKk45eSWlM"
      },
      "source": [
        "def numeric_clean_data():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/numeric_clean_data.csv','w') as numeric_csv_file:\n",
        "    csv_writer = csv.writer(numeric_csv_file)\n",
        "    csv_writer.writerow(['hindi','english'])\n",
        "    list_of_lists = []\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[0]\n",
        "        english_sen = line[1]\n",
        "        tokens = indicnlp_hindi_tokenizer(hindi_sen)\n",
        "        hindi_ind_ls = [hindi_tok_to_ind_dict[token] for token in tokens]\n",
        "        while len(hindi_ind_ls) > maximum_length:\n",
        "          hindi_ind_ls.pop()\n",
        "        while len(hindi_ind_ls) < (maximum_length):\n",
        "          hindi_ind_ls.append(0)\n",
        "        tokens = indicnlp_hindi_tokenizer(english_sen)\n",
        "        english_ind_ls = [english_tok_to_ind_dict[token] for token in tokens]\n",
        "        while len(english_ind_ls) > maximum_length:\n",
        "          english_ind_ls.pop()\n",
        "        while len(english_ind_ls) < (maximum_length):\n",
        "          english_ind_ls.append(0)\n",
        "        list_of_lists.append([hindi_ind_ls,english_ind_ls])\n",
        "        csv_writer.writerow([hindi_ind_ls,english_ind_ls])\n",
        "    return list_of_lists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAToMOeSUmOp"
      },
      "source": [
        "clean_numeric = numeric_clean_data()\n",
        "clean_np = torch.tensor(clean_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFYr9SKH6yVR"
      },
      "source": [
        "clean_np.shape[0]\n",
        "clean_np[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3lEs0wjIS-r"
      },
      "source": [
        "## Creating enumerable data set using Dataset class for DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfVYDimL_HJX"
      },
      "source": [
        "class hindi_english_train_dataset(Dataset):\n",
        "  def __init__(self):\n",
        "    clean_np = torch.tensor(clean_numeric)\n",
        "    self.num_samples = clean_np.shape[0]\n",
        "    self.hindi_sen = clean_np[:,0]\n",
        "    self.english_sen = clean_np[:,1]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      return self.hindi_sen[index], self.english_sen[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.num_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek-nlTqcdAVg"
      },
      "source": [
        "## Sample Testing the iterable data set created in above cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbQ-0rzcQfi_"
      },
      "source": [
        "train_data = hindi_english_train_dataset()\n",
        "hindi_sen,english_sen = train_data[1]\n",
        "print(hindi_sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20zhiwtkJtjC"
      },
      "source": [
        "## Setting Seq to Seq Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5UB54mvJp7O"
      },
      "source": [
        "# input size to encoder\n",
        "hindi_one_hot_len = len(hindi_tok_to_ind_dict)\n",
        "# input size and output size to decoder (both should be same as output of decoder is fed to decoder)\n",
        "english_one_hot_len = len(english_tok_to_ind_dict)\n",
        "output_size = len(english_tok_to_ind_dict)\n",
        "\n",
        "# hidden size\n",
        "hidden_size = 1024\n",
        "# number of epochs to be trained\n",
        "num_epochs = 70\n",
        "# learning rate\n",
        "learning_rate = 0.001\n",
        "# batch size\n",
        "batch_size = 128\n",
        "#drop out rates for encoder & decoder\n",
        "dropout_p = 0\n",
        "\n",
        "# number of layers for encode & decoder\n",
        "num_layers = 1\n",
        "\n",
        "# encoder embedding size\n",
        "encoder_embedding_size = 300\n",
        "# decoder embedding size\n",
        "decoder_embedding_size = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cIFiT1wctki"
      },
      "source": [
        "## Creating iterator for training data using DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1HcdYbQ_PdJ"
      },
      "source": [
        "train_iterator = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNG6UsyBcD1u"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fieqzVHLygV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,num_layers,dropout_p=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropout_p,bidirectional=True)\n",
        "\n",
        "  def forward(self,sample):\n",
        "    # print('sample'); print(sample[:,0])\n",
        "    embedding = self.embedding(sample)\n",
        "    embedding_withdropout = self.dropout(embedding)\n",
        "    encoder_outputs,(hidden,cell) = self.rnn(embedding_withdropout)\n",
        "    # print('encoder')\n",
        "    # print(encoder_outputs.shape)\n",
        "    # print(hidden.shape)\n",
        "    hidden = 0.5*(hidden[0] + hidden[1])\n",
        "    cell = 0.5*(cell[0]+cell[1])\n",
        "    # encoder_outputs = 0.5*(encoder_outputs[0]+encoder_outputs[1])\n",
        "    hidden = hidden.unsqueeze(0)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    # encoder_outputs = encoder_outputs.unsqueeze(0)\n",
        "    return encoder_outputs,hidden,cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUemrLEJsJV-"
      },
      "source": [
        "## Attention mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kG5Q7nBV6Uyz"
      },
      "source": [
        "def attention_mech(encoder_outputs,dec_prev_hidden):\n",
        "  # print('attention')\n",
        "  dec_prev_hidden = torch.cat((dec_prev_hidden,dec_prev_hidden),dim=2)\n",
        "  # print(dec_prev_hidden.shape)\n",
        "  # encoder_outputs = encoder_outputs.permute(0,2,1)\n",
        "  attention_score = dec_prev_hidden*encoder_outputs\n",
        "  # print(attention_score.shape)\n",
        "  attention_score = torch.sum(attention_score,dim=-1)\n",
        "  # print(attention_score.shape)\n",
        "  attention_score = attention_score.unsqueeze(2)\n",
        "  # print(attention_score.shape)\n",
        "  attention_weights = torch.softmax(attention_score,dim=2)\n",
        "  context_vector = attention_weights*encoder_outputs\n",
        "  context_vector = torch.sum(context_vector,dim=0)\n",
        "  context_vector = context_vector.unsqueeze(0)\n",
        "  # print(context_vector.shape)\n",
        "  return context_vector,attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty3rLRJAcGSs"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cIkBH0bMtUG"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,dropout_p=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    self.embedding_size= embedding_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size,self.embedding_size)\n",
        "    self.rnn = nn.LSTM((self.embedding_size+2*self.hidden_size),hidden_size,num_layers,dropout=dropout_p)\n",
        "    self.fc = nn.Linear(self.hidden_size,self.output_size)\n",
        "\n",
        "  def forward(self,x,hidden,cell,encoder_outputs):\n",
        "    x = x.unsqueeze(0)\n",
        "  \n",
        "    embedding = self.embedding(x)\n",
        "    embedding_dropout = self.dropout(embedding)\n",
        "    # print(embedding_dropout.shape)\n",
        "    context_vector,attention_weights = attention_mech(encoder_outputs,hidden)\n",
        "  \n",
        "    rnn_input = torch.cat((context_vector,embedding_dropout),dim=2)\n",
        "    outputs, (hidden,cell) = self.rnn(rnn_input,(hidden,cell))\n",
        "    y_hats = self.fc(outputs)\n",
        "    \n",
        "    y_hats = y_hats.squeeze(0)\n",
        "   \n",
        "    return y_hats,hidden,cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vKkU3ZNcKFQ"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUNBCO3BMxln"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super(Seq2Seq,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,hindi_batch,english_batch,teacher_force_ratio = 0.5):\n",
        "    batch_size = hindi_batch.shape[1]\n",
        "    english_batch_len = english_batch.shape[0]\n",
        "    english_vocab_size = len(english_tok_to_ind_dict)\n",
        "\n",
        "    outputs = torch.zeros(english_batch_len,batch_size,english_vocab_size).to(device)\n",
        "    encoder_outputs,hidden,cell = self.encoder(hindi_batch)\n",
        "\n",
        "    sample=english_batch[0]\n",
        "\n",
        "    for token in range(1,english_batch_len):\n",
        "      predict,hidden,cell = self.decoder(sample,hidden,cell,encoder_outputs)\n",
        "      outputs[token] = predict\n",
        "      probable_token = predict.argmax(1)\n",
        "      sample = english_batch[token] if random.random() < teacher_force_ratio else probable_token\n",
        "     \n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teYdxM15E30I"
      },
      "source": [
        "## Model Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPYsjnze_8wz"
      },
      "source": [
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -0.5, 0.5)\n",
        "    # nn.init.xavier_uniform_(param.data, gain=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNnrlJCwplfP"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70aq4gJKcgox"
      },
      "source": [
        "encoder = Encoder(hindi_one_hot_len,encoder_embedding_size,\n",
        "                      hidden_size,num_layers,dropout_p).to(device)\n",
        "\n",
        "decoder = Decoder(english_one_hot_len,decoder_embedding_size,\n",
        "                      hidden_size,output_size,num_layers,dropout_p).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder,decoder).to(device)\n",
        "\n",
        "pad_index = english_tok_to_ind_dict['pad']\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
        "# init_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5GUJ1zMAtN"
      },
      "source": [
        "## Using Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7ooP-7MGni"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Jr2hAKE7vY"
      },
      "source": [
        "## Clipping gradient function to avoid exploding gradient problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9iqPLp8E-4Q"
      },
      "source": [
        "def clip_gradients(model):\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s9cEteaaEKa"
      },
      "source": [
        "## Function to insert start and end tokens at the begining and end of a token list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COvBzI7cY2rQ"
      },
      "source": [
        "def insert_sos_eos(tokens):\n",
        "  tokens.insert(0, 'sos')\n",
        "  tokens.append('eos')\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYIn9P1OZ105"
      },
      "source": [
        "## Function that output's indices for a list of tokens from Hinidi Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayJlLXiiZWYf"
      },
      "source": [
        "def token_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    if token in hindi_tok_to_ind_dict:\n",
        "      indices.append(hindi_tok_to_ind_dict[token])\n",
        "  return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E2yQkO7gFLO"
      },
      "source": [
        "## Function that output's tokens for a list of indices from English Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZypnML0ydm-t"
      },
      "source": [
        "def index_to_token(indices):\n",
        "  english_sentence = []\n",
        "  for index in indices:\n",
        "    english_sentence.append(english_ind_to_tok_dict[index])\n",
        "  return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NVCQtdWswdR"
      },
      "source": [
        "def hindi_to_english(model, text,device, max_length=15):\n",
        "    tokens = indicnlp_hindi_tokenizer(text)\n",
        "    tokens = insert_sos_eos(tokens)\n",
        "    indices_for_tokens = token_to_index(tokens)\n",
        "    ## unsqueeze the index list of input sentence to get a batch of single sentence\n",
        "    hindi_batch = torch.LongTensor(indices_for_tokens).unsqueeze(1).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs,hidden, cell = model.encoder(hindi_batch)\n",
        "\n",
        "    decoder_output = [english_tok_to_ind_dict[\"sos\"]]\n",
        "\n",
        "    for word in range(max_length):\n",
        "        last_inp_decoder = torch.LongTensor([decoder_output[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            current_output, hidden, cell = model.decoder(last_inp_decoder, hidden, cell,encoder_outputs)\n",
        "\n",
        "        decoder_output.append(current_output.argmax(1).item())\n",
        "        if current_output.argmax(1).item() == english_tok_to_ind_dict[\"eos\"]:\n",
        "            break\n",
        "\n",
        "    english_sentence = index_to_token(decoder_output)\n",
        "    english_sentence.remove('sos')\n",
        "    if english_sentence[-1]=='eos':\n",
        "      english_sentence.pop()\n",
        "    return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuJ6UqCNczDr"
      },
      "source": [
        "## Training and Checking the loss on Validation data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilen4WLjetwq"
      },
      "source": [
        "training_loss = []\n",
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा'\n",
        "min_loss = 10000\n",
        "for epoch in range(num_epochs):\n",
        "  batch_num = 1\n",
        "  # torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch_W3_2.pth')\n",
        "  model.train(True)\n",
        "  training_loss = []\n",
        "  # print(batch_num)\n",
        "  for batch_index,(hindi_batch,english_batch) in enumerate(train_iterator):\n",
        "    # print(batch_num)\n",
        "    hindi_batch = torch.transpose(hindi_batch, 0, 1).to(device)\n",
        "    english_batch = torch.transpose(english_batch, 0, 1).to(device)\n",
        "\n",
        "    # print('inputdata:');print(hindi_batch.shape);print('outputdata:');print(english_batch.shape)\n",
        "    # print(hindi_batch[:,0]);print(hindi_batch[:,1]);print(hindi_batch[:,2]);print(hindi_batch[:,3]);print(hindi_batch[:,4])\n",
        "    # print(english_batch[:,0]);print(english_batch[:,1]);print(english_batch[:,2]);print(english_batch[:,3]);print(english_batch[:,4]);\n",
        "    output = model(hindi_batch,english_batch)\n",
        "\n",
        "    output = output[1:].reshape(-1,output.shape[2])\n",
        "    english_batch = english_batch[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterian(output,english_batch)\n",
        "    training_loss.append(loss)\n",
        "    if loss.item() < min_loss:\n",
        "      min_loss = loss.item()\n",
        "      # torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_with_leastloss_W3.pth')\n",
        "    # print('loss'+str(loss))\n",
        "    # print('batch_num:'+str(batch_num))\n",
        "    loss.backward()\n",
        "    clip_gradients(model)\n",
        "    optimizer.step()\n",
        "    batch_num+=1\n",
        "  T_loss = sum(training_loss)\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], training loss: {T_loss/len(train_iterator)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGnIS9qNz1x"
      },
      "source": [
        "# torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/afterepochs_apr03_sub2.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5nERqBcr_X"
      },
      "source": [
        "## Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBAif8_ejUS"
      },
      "source": [
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch_W3.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzJkMCQwan6"
      },
      "source": [
        "f = open('/content/drive/MyDrive/Colab_Notebooks/answer_test_week3.txt',\"w\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhlU-wNhUAvG"
      },
      "source": [
        "model.eval()\n",
        "i = 1\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/clean_test.csv','r') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  next(csv_reader)\n",
        "  for line in csv_reader:\n",
        "    sentence = line[0]\n",
        "    english_sentence = hindi_to_english(model,sentence,device,max_length=40)\n",
        "    output_sen = ' '.join(map(str, english_sentence))\n",
        "    output_sen = output_sen + '\\n'\n",
        "    f.write(output_sen)\n",
        "    print(i)\n",
        "    i=i+1\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12cPE-oRjg6n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}