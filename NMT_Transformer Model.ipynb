{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hind_eng_NMT_20111070_final.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuLbPWyaagf"
      },
      "source": [
        "### For running the notebook, create a directory with name: \"Colab_Notebooks\" in Google Drive\n",
        "Then save the data files \"train.csv\" & \"testhindistatements.csv\" in that \"Colab_Notebooks\" directory. The output file \"answer.txt\", \".pth file\" and other pre-processed files will be saved in the same folder: \"Colab_Notebooks\" "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_caFAenHP-d"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoL9dmHeAMCD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MA9X771Hp0d"
      },
      "source": [
        "## Installing indicnlp module for tokenizing hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOnqfy0PE5OD"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-00pO88Fe16I"
      },
      "source": [
        "## Importing Python dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mL4oItNBvSs"
      },
      "source": [
        "import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"   # Used for debugging cuda device errors. It forces synchronus execution b/n host & device.\n",
        "import csv\n",
        "import numpy as np\n",
        "import  re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "# For tokenizing english sentence\n",
        "import spacy\n",
        "# For tokenizing hindi sentence  \n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU2g5gTVFhrd"
      },
      "source": [
        "## Setting up the device based on availabilty of GPU\n",
        "The name of GPU assigned is printed after this cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adyBDgMY-mgm"
      },
      "source": [
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('The GPU allocated is:',torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMSzH2oFtt4"
      },
      "source": [
        "## Load default spacy english-core-web model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcyNDKXDd4q"
      },
      "source": [
        "spacy_english = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RpOzneLGFxD"
      },
      "source": [
        "## Function that returns list of tokens in an english sentence using sacy tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1hVyxIABZl"
      },
      "source": [
        "def spacy_english_tokenizer(sentence):\n",
        "  tokenized_sent = spacy_english.tokenizer(sentence)\n",
        "  token_list = []\n",
        "  for token in tokenized_sent:\n",
        "    token_list.append(token.text)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00eMgbiTGE-I"
      },
      "source": [
        "## Function that returns list of tokens in a hindi sentence using indicnlp tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gspj8cvvFroQ"
      },
      "source": [
        "def indicnlp_hindi_tokenizer(sentence):\n",
        "  token_list = indic_tokenize.trivial_tokenize(sentence)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO9tggsrGVrG"
      },
      "source": [
        "## Testing english tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2iQlJKxyWA"
      },
      "source": [
        "sentence = 'hello world! Here i come'\n",
        "# spacy_english_tokenizer(sentence)\n",
        "indicnlp_hindi_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lLvDZkGbcR"
      },
      "source": [
        "## Testing hindi tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUHY4pjQyk5A"
      },
      "source": [
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा।'\n",
        "indicnlp_hindi_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR021es3hLVB"
      },
      "source": [
        "## Maximum length of sentences we will be considering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgkv7pQ1d3zE"
      },
      "source": [
        "maximum_length = 15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutXBe6Lv2I3"
      },
      "source": [
        "## Function that cleans Train data and saves the clean train data to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM6mZvvSv1UO"
      },
      "source": [
        "def clean_train_data():\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  num_lines = 0\n",
        "  avg_length = 0\n",
        "  maxi_length = 0\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','w') as clean_csv_file:  # Opening a csv file to save clean train data\n",
        "    csv_writer = csv.writer(clean_csv_file)\n",
        "    csv_writer.writerow(['hindi','english'])\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/train.csv','r') as csv_file: # Opening train.csv file\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        num_lines+=1\n",
        "        avg_length += len(line[1].split()) \n",
        "        if len(line[1].split()) > maxi_length:\n",
        "          maxi_length = len(line[1].split()) # finding longest hindi sentence in train set\n",
        "        hindi_sen = line[1]\n",
        "        english_sen = line[2]\n",
        "        if len(hindi_sen) == 0 or len(english_sen) ==0:   # skipping lines from train.csv if they don't contain hindi or english sentence\n",
        "          continue\n",
        "        if len(hindi_sen) > 1.5* len(english_sen) or len(english_sen) > 1.5* len(hindi_sen): # skipping sentences if length of hindi and englisj sentences are not similar  \n",
        "          continue\n",
        "        \n",
        "        if len(english_sen.split())>maximum_length or len(hindi_sen.split())>maximum_length: # skipping sentences that are very long\n",
        "          continue\n",
        "        \n",
        "        english_sen = english_sen.strip()\n",
        "        english_sen = normalize('NFD',english_sen).encode('ascii', 'ignore') # normailsing english text to NFD form\n",
        "        english_sen = english_sen.decode('UTF-8')\n",
        "        temp_str_words = english_sen.split()\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words] # removing punctuation from english text\n",
        "        \n",
        "        temp_str_words = [word.lower() for word in temp_str_words] # lower casing english text\n",
        "        temp_str_words = [word for word in temp_str_words if word.isalpha()]\n",
        "        \n",
        "        temp_str_words.insert(0,'sos'); temp_str_words.append('eos') # appending sos, eos tokens to english sentences\n",
        "        english_sen = ' '.join(map(str, temp_str_words))\n",
        "\n",
        "        hindi_sen = hindi_sen.strip()\n",
        "        hindi_sen.replace('♪', ''); hindi_sen.replace('♫', '') # removing musical symbols from hindi sentences\n",
        "        hindi_sen = normalize('NFD',hindi_sen).encode('UTF-8', 'ignore') # normalising hindi text\n",
        "        hindi_sen = hindi_sen.decode('UTF-8')\n",
        "        temp_str_words = hindi_sen.split()\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words] # removing punctuation from hindi text\n",
        "        \n",
        "        temp_str_words = [word.lower() for word in temp_str_words]\n",
        "        temp_str_words = [word for word in temp_str_words if not (word==' ')]\n",
        "       \n",
        "        temp_str_words.insert(0,'sos'); temp_str_words.append('eos') # appending sos and eos to hindi sentences\n",
        "        hindi_sen = ' '.join(map(str, temp_str_words))\n",
        "        csv_writer.writerow([hindi_sen,english_sen])\n",
        "\n",
        "  return (avg_length//num_lines),maxi_length # finding avg length of hindi sentences in train data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17msSSAyzaO"
      },
      "source": [
        "## Cleaning the Training data and printing train data set statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZujINLkFyuv7"
      },
      "source": [
        "train_hindi_sen_avg_length,train_hindi_sen_max_length = clean_train_data()\n",
        "print('traindata_hindi_sentence_avg_length: '+str(train_hindi_sen_avg_length))\n",
        "print('traindata_hindi_sentence_max_length: '+str(train_hindi_sen_max_length))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiBSEo5xzFBQ"
      },
      "source": [
        "## Function that cleans test data and saves it to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5oyL4EvzMpT"
      },
      "source": [
        "def clean_test_data():\n",
        "   table = str.maketrans('', '', string.punctuation)\n",
        "   num_lines = 0\n",
        "   avg_length = 0\n",
        "   maxi_length = 0\n",
        "   with open('/content/drive/MyDrive/Colab_Notebooks/clean_test.csv','w') as clean_csv_file: # opening a csv file to save clean test data\n",
        "    csv_writer = csv.writer(clean_csv_file)\n",
        "    csv_writer.writerow(['hindi'])\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/testhindistatements.csv','r') as csv_file: # opening hindistatements.csv\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        num_lines+=1\n",
        "        hindi_sen = line[2]\n",
        "        hindi_sen = hindi_sen.strip()\n",
        "        hindi_sen.replace('♪', ''); hindi_sen.replace('♫', '')\n",
        "        hindi_sen = normalize('NFD',hindi_sen).encode('UTF-8', 'ignore') # normailsing test data\n",
        "        hindi_sen = hindi_sen.decode('UTF-8')\n",
        "        temp_str_words = hindi_sen.split()\n",
        "        avg_length += len(temp_str_words)\n",
        "        if len(temp_str_words) > maxi_length:\n",
        "          maxi_length = len(temp_str_words)\n",
        "        temp_str_words = [word.translate(table) for word in temp_str_words] # removing punctuation\n",
        "        \n",
        "        temp_str_words = [word.lower() for word in temp_str_words]\n",
        "        temp_str_words = [word for word in temp_str_words if word != ' ']\n",
        "        hindi_sen = ' '.join(map(str, temp_str_words))\n",
        "\n",
        "        csv_writer.writerow([hindi_sen])\n",
        "    return (avg_length//num_lines),maxi_length # finding avg length of hindi sentence in test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdo_MMw-y_00"
      },
      "source": [
        "## Cleaning the Test data and printing test data statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA3Vy7n-y-6f"
      },
      "source": [
        "avg_hindi_sen_test_len,max_hindi_sen_test_len = clean_test_data()\n",
        "print('avg_hindi_sen_test_len: '+str(avg_hindi_sen_test_len))\n",
        "print('max_hindi_sen_test_len: '+str(max_hindi_sen_test_len))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3s_0q9RMh9zk"
      },
      "source": [
        "## Creating dictionaries for hindi and english vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X_Is3Qp8Dun"
      },
      "source": [
        "hindi_tok_to_ind_dict = {'pad':0, 'sos':1, 'eos':2} # creating hindi token to index dictinory\n",
        "hindi_ind_to_tok_dict = {0:'pad', 1:'sos', 2:'eos'} # creating hindi index to token dictionary\n",
        "english_tok_to_ind_dict = {'pad':0, 'sos':1, 'eos':2} # creating english token to index dictionary\n",
        "english_ind_to_tok_dict = {0:'pad', 1:'sos', 2:'eos'} # creating english index to token dictionary "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMBaSSQFh86S"
      },
      "source": [
        "## Creating Hindi vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miGle4_k4eok"
      },
      "source": [
        "def generate_hindi_vocab():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      index_count = 3\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[0]\n",
        "        tokens = indicnlp_hindi_tokenizer(hindi_sen)\n",
        "        for token in tokens:\n",
        "          if token not in hindi_tok_to_ind_dict:\n",
        "            hindi_tok_to_ind_dict[token] = index_count\n",
        "            hindi_ind_to_tok_dict[index_count] = token\n",
        "            index_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW7fP2Ln7gd2"
      },
      "source": [
        "generate_hindi_vocab()\n",
        "print('size of hindi vocab: '+str(len(hindi_ind_to_tok_dict)))\n",
        "print(len(hindi_tok_to_ind_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI0d6Rn9iUtI"
      },
      "source": [
        "## Generating English Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0m93nJl8U_W"
      },
      "source": [
        "def generate_english_vocab():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      index_count = 3\n",
        "      for line in csv_reader:\n",
        "        english_sen = line[1]\n",
        "        tokens = indicnlp_hindi_tokenizer(english_sen)\n",
        "        for token in tokens:\n",
        "          if token not in english_tok_to_ind_dict:\n",
        "            english_tok_to_ind_dict[token] = index_count\n",
        "            english_ind_to_tok_dict[index_count] = token\n",
        "            index_count+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIjqN1BF9Xyb"
      },
      "source": [
        "generate_english_vocab()\n",
        "print('Size of English Vocabulary: '+str(len(english_ind_to_tok_dict)))\n",
        "print(len(english_tok_to_ind_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wp2O0lTtfpx7"
      },
      "source": [
        "## Replacing tokens in train data with their corresponding indices from Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-xKk45eSWlM"
      },
      "source": [
        "def numeric_clean_data():\n",
        "  with open('/content/drive/MyDrive/Colab_Notebooks/numeric_clean_data.csv','w') as numeric_csv_file:\n",
        "    csv_writer = csv.writer(numeric_csv_file)\n",
        "    csv_writer.writerow(['hindi','english'])\n",
        "    list_of_lists = []\n",
        "    with open('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv','r') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "      next(csv_reader)\n",
        "      for line in csv_reader:\n",
        "        hindi_sen = line[0]\n",
        "        english_sen = line[1]\n",
        "        tokens = indicnlp_hindi_tokenizer(hindi_sen)\n",
        "        hindi_ind_ls = [hindi_tok_to_ind_dict[token] for token in tokens]\n",
        "        while len(hindi_ind_ls) > maximum_length:\n",
        "          hindi_ind_ls.pop()\n",
        "        while len(hindi_ind_ls) < (maximum_length):\n",
        "          hindi_ind_ls.append(0)\n",
        "        tokens = indicnlp_hindi_tokenizer(english_sen)\n",
        "        english_ind_ls = [english_tok_to_ind_dict[token] for token in tokens]\n",
        "        while len(english_ind_ls) > maximum_length:\n",
        "          english_ind_ls.pop()\n",
        "        while len(english_ind_ls) < (maximum_length):\n",
        "          english_ind_ls.append(0)\n",
        "        list_of_lists.append([hindi_ind_ls,english_ind_ls])\n",
        "        csv_writer.writerow([hindi_ind_ls,english_ind_ls])\n",
        "    return list_of_lists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAToMOeSUmOp"
      },
      "source": [
        "clean_numeric = numeric_clean_data()\n",
        "clean_np = torch.tensor(clean_numeric)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFYr9SKH6yVR"
      },
      "source": [
        "clean_np.shape[0]\n",
        "clean_np[0,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3lEs0wjIS-r"
      },
      "source": [
        "## Making iterable data set using Dataset for Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfVYDimL_HJX"
      },
      "source": [
        "class hindi_english_train_dataset(Dataset):\n",
        "  def __init__(self):\n",
        "    clean_np = torch.tensor(clean_numeric)\n",
        "    self.num_samples = clean_np.shape[0]\n",
        "    self.hindi_sen = clean_np[:,0]\n",
        "    self.english_sen = clean_np[:,1]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "      return self.hindi_sen[index], self.english_sen[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.num_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbQ-0rzcQfi_"
      },
      "source": [
        "train_data = hindi_english_train_dataset()\n",
        "hindi_sen,english_sen = train_data[1]\n",
        "print(hindi_sen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20zhiwtkJtjC"
      },
      "source": [
        "## Setting Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5UB54mvJp7O"
      },
      "source": [
        "# input size to encoder\n",
        "hindi_one_hot_len = len(hindi_tok_to_ind_dict)\n",
        "# input size and output size to decoder (both should be same as output of decoder is fed to decoder)\n",
        "english_one_hot_len = len(english_tok_to_ind_dict)\n",
        "\n",
        "num_heads = 8\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = 2\n",
        "\n",
        "# number of epochs to be trained\n",
        "num_epochs = 70\n",
        "\n",
        "# learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "# batch size\n",
        "batch_size = 128\n",
        "\n",
        "#drop out rates for encoder & decoder\n",
        "dropout = 0.1\n",
        "\n",
        "max_len = maximum_length\n",
        "forward_expansion = 4\n",
        "pad_index = hindi_tok_to_ind_dict[\"pad\"]\n",
        "\n",
        "# embedding size\n",
        "embedding_size = 512"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXmsKPtgf5Oc"
      },
      "source": [
        "## Creating iterator for train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1HcdYbQ_PdJ"
      },
      "source": [
        "train_iterator = DataLoader(dataset=train_data,batch_size=batch_size,shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vByfrKBef_4Q"
      },
      "source": [
        "## Function that generates Sinusoidal Positional Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_NLHa74OGnK"
      },
      "source": [
        "def create_sinusoidal_Embeddings(maximum_sequence_length,embedding_size):\n",
        "    temp = np.array([[position / np.power(10000, 2 * (dim // 2) /embedding_size) for dim in range(embedding_size)] for position in range(maximum_sequence_length)])\n",
        "    pos_embeddings = torch.zeros(temp.shape)\n",
        "    pos_embeddings[:, 0::2] = torch.FloatTensor(np.sin(temp[:, 0::2]))\n",
        "    pos_embeddings[:, 1::2] = torch.FloatTensor(np.cos(temp[:, 1::2]))\n",
        "    return pos_embeddings.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPC3D0YNgHka"
      },
      "source": [
        "## Function That generates Positional embeddings based on nn.Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDmmEyYh2vww"
      },
      "source": [
        "class create_Embeddings(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_size,maximum_sequence_length):\n",
        "    super(create_Embeddings,self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.token_embedding = nn.Embedding(vocab_size,embedding_size)\n",
        "    self.position_embedding = nn.Embedding(maximum_sequence_length,embedding_size)\n",
        "\n",
        "  def forward(self,source,source_sequence_ids):\n",
        "    ## Option1: choose fully connected netwrok to generate positional embeddings.\n",
        "    return (self.token_embedding(source)+self.position_embedding(source_sequence_ids))\n",
        "    ## Option2: choose sinusoidal positional embeddings.\n",
        "    #temp = create_sinusoidal_Embeddings(source.shape[0],self.embedding_size).unsqueeze(1).expand(source.shape[0],source.shape[1],self.embedding_size)\n",
        "    #return (self.token_embedding(source)+temp) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJRub_NqgTr3"
      },
      "source": [
        "## Function that generates Target Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tloGetkMdQ15"
      },
      "source": [
        "def generate_target_mask(target_sequence_length):\n",
        "  mask = (torch.tril(torch.ones(target_sequence_length, target_sequence_length)) == 1)\n",
        "  return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0)).to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9ndtM_PgYI0"
      },
      "source": [
        "## Function that generates source mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrbotnWueHJ4"
      },
      "source": [
        "def generate_source_mask(hindi_batch,pad_index):\n",
        "  return (hindi_batch.transpose(0, 1) == pad_index).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ2AB2QHi6Lg"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4DS_sFEdVQ4"
      },
      "source": [
        "class Transformer_NMT(nn.Module):\n",
        "    def __init__(self, hindi_vocab_size, english_vocab_size,embedding_size,pad_index,num_heads,num_encoder_layers,\n",
        "                 num_decoder_layers,forward_expansion,dropout_p,maximum_sequence_length):\n",
        "        super(Transformer_NMT, self).__init__()\n",
        "        self.hindi_embedding = create_Embeddings(hindi_vocab_size, embedding_size,maximum_sequence_length)\n",
        "        self.english_embedding = create_Embeddings(english_vocab_size, embedding_size,maximum_sequence_length)\n",
        "        self.transformer = nn.Transformer(d_model=embedding_size,nhead=num_heads,num_encoder_layers=num_encoder_layers,num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=forward_expansion,dropout=dropout_p) #activation='relu'  activation='gelu'\n",
        "        \n",
        "        self.fc_out = nn.Linear(embedding_size, english_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, hindi_batch, english_batch):\n",
        "        hindi_sequence_length, N = hindi_batch.shape\n",
        "        english_sequence_length, N = english_batch.shape\n",
        "        hindi_sequence_positions = torch.arange(0, hindi_sequence_length).unsqueeze(1).expand(hindi_sequence_length, N).to(device)\n",
        "        english_sequence_positions = torch.arange(0, english_sequence_length).unsqueeze(1).expand(english_sequence_length, N).to(device)\n",
        "\n",
        "        hindi_batch_embedding = self.hindi_embedding(hindi_batch,hindi_sequence_positions)\n",
        "        english_batch_embedding = self.english_embedding(english_batch,english_sequence_positions)\n",
        "        \n",
        "        hindi_sen_pad_mask = generate_source_mask(hindi_batch,pad_index).to(device)\n",
        "        english_sentence_mask = generate_target_mask(english_sequence_length).to(device)\n",
        "\n",
        "        prediction = self.transformer(hindi_batch_embedding,english_batch_embedding,src_key_padding_mask=hindi_sen_pad_mask,tgt_mask=english_sentence_mask)\n",
        "        prediction = self.fc_out(prediction)\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teYdxM15E30I"
      },
      "source": [
        "## Model Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPYsjnze_8wz"
      },
      "source": [
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -1, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNnrlJCwplfP"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70aq4gJKcgox"
      },
      "source": [
        "\n",
        "model = Transformer_NMT(hindi_one_hot_len,english_one_hot_len,embedding_size,pad_index,num_heads,num_encoder_layers,\n",
        "                        num_decoder_layers,forward_expansion,dropout,max_len).to(device)\n",
        "\n",
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/after35epochs_final.pth')\n",
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch_W5_1.pth')\n",
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/after70epochs_final.pth')\n",
        "\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
        "# init_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5GUJ1zMAtN"
      },
      "source": [
        "## Using Adam Optimizer and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7ooP-7MGni"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Jr2hAKE7vY"
      },
      "source": [
        "## Clipping gradient function to avoid exploding gradient problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9iqPLp8E-4Q"
      },
      "source": [
        "def clip_gradients(model):\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s9cEteaaEKa"
      },
      "source": [
        "## Function to insert start and end tokens at the begining and end of a token list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COvBzI7cY2rQ"
      },
      "source": [
        "def insert_sos_eos(tokens):\n",
        "  tokens.insert(0, 'sos')\n",
        "  tokens.append('eos')\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYIn9P1OZ105"
      },
      "source": [
        "## Function that output's indices for a list of tokens from Hinidi Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayJlLXiiZWYf"
      },
      "source": [
        "def token_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    if token in hindi_tok_to_ind_dict:\n",
        "      indices.append(hindi_tok_to_ind_dict[token])\n",
        "  return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E2yQkO7gFLO"
      },
      "source": [
        "## Function that output's tokens for a list of indices from English Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZypnML0ydm-t"
      },
      "source": [
        "def index_to_token(indices):\n",
        "  english_sentence = []\n",
        "  for index in indices:\n",
        "    english_sentence.append(english_ind_to_tok_dict[index])\n",
        "  return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odoUmb8peahm"
      },
      "source": [
        "## Function that translates hindi test sentences to English sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NVCQtdWswdR"
      },
      "source": [
        "def hindi_to_english(model, text,device, max_length=15):\n",
        "    tokens = indicnlp_hindi_tokenizer(text)\n",
        "    tokens = insert_sos_eos(tokens)\n",
        "    indices_for_tokens = token_to_index(tokens)\n",
        "    if len(indices_for_tokens) > maximum_length:\n",
        "      indices_for_tokens = indices_for_tokens[0:maximum_length]\n",
        "    \n",
        "    hindi_batch = torch.LongTensor(indices_for_tokens).unsqueeze(1).to(device)\n",
        "    \n",
        "    decoder_output = [english_tok_to_ind_dict[\"sos\"]]\n",
        "\n",
        "    for word in range(max_length):\n",
        "        last_inp_decoder = torch.LongTensor(decoder_output).unsqueeze(1).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            current_output = model(hindi_batch,last_inp_decoder)\n",
        "\n",
        "        decoder_output.append(current_output.argmax(2)[-1,:].item())\n",
        "        if current_output.argmax(2)[-1,:].item() == english_tok_to_ind_dict[\"eos\"]:\n",
        "            break\n",
        "\n",
        "    english_sentence = index_to_token(decoder_output)\n",
        "    english_sentence.remove('sos')\n",
        "    if english_sentence[-1]=='eos':\n",
        "      english_sentence.pop()\n",
        "    return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuJ6UqCNczDr"
      },
      "source": [
        "## Training and Checking the loss on Train data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilen4WLjetwq"
      },
      "source": [
        "training_loss = []\n",
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा'\n",
        "min_loss = 10000\n",
        "for epoch in range(num_epochs):\n",
        "  batch_num = 1\n",
        "  # torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch.pth')\n",
        "  model.train(True)\n",
        "  training_loss = []\n",
        "  for batch_index,(hindi_batch,english_batch) in enumerate(train_iterator):\n",
        "    # print(batch_num)\n",
        "    hindi_batch = torch.transpose(hindi_batch, 0, 1).to(device)\n",
        "    english_batch = torch.transpose(english_batch, 0, 1).to(device)\n",
        "    \n",
        "    output = model(hindi_batch,english_batch[:-1, :])\n",
        "    output = output.reshape(-1,output.shape[2])\n",
        "    english_batch = english_batch[1:].reshape(-1)  ##flatening the tensor into a vector\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss = criterian(output,english_batch)  \n",
        "    training_loss.append(loss)\n",
        "    if loss.item() < min_loss:\n",
        "      min_loss = loss.item()\n",
        "      # torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_with_leastloss.pth')\n",
        "    # print('loss'+str(loss))\n",
        "    # print('batch_num:'+str(batch_num))\n",
        "    loss.backward()\n",
        "    clip_gradients(model)\n",
        "    optimizer.step()\n",
        "    # scheduler.step()\n",
        "    batch_num+=1\n",
        "    \n",
        "  T_loss = sum(training_loss)\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], training loss: {T_loss/len(train_iterator)}')\n",
        "  # if loss < 0.0005:\n",
        "    # break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGnIS9qNz1x"
      },
      "source": [
        "# torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/after70epochs_final.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBAif8_ejUS"
      },
      "source": [
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch_W4.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCHiwiI2d1oH"
      },
      "source": [
        "## Opening a text file to store translated sentences for test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzJkMCQwan6"
      },
      "source": [
        "f = open('/content/drive/MyDrive/Colab_Notebooks/answer.txt',\"w\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agTSe6atd8nF"
      },
      "source": [
        "## Translating the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhlU-wNhUAvG"
      },
      "source": [
        "model.eval()\n",
        "i = 1\n",
        "with open('/content/drive/MyDrive/Colab_Notebooks/clean_test.csv','r') as csv_file:\n",
        "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "  next(csv_reader)\n",
        "  for line in csv_reader:\n",
        "    sentence = line[0]\n",
        "    english_sentence = hindi_to_english(model,sentence,device,max_length=maximum_length)\n",
        "    output_sen = ' '.join(map(str, english_sentence))\n",
        "    output_sen = output_sen + '\\n'\n",
        "    f.write(output_sen)\n",
        "    print(i)\n",
        "    i=i+1\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF9OGDalglOf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}