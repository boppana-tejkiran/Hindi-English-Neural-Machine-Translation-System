{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hind_eng_NMT_Seq2Seq_Model_Week2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GuLbPWyaagf"
      },
      "source": [
        "# For running the notebook, create a directory with name: \"Colab_Notebooks\" in Google Drive\n",
        "# Then save the data files \"train.csv\" & \"hindistatements.csv\" in the Colab_Notebooks directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_caFAenHP-d"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoL9dmHeAMCD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUB12htcqU9W"
      },
      "source": [
        "# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwAA14CIHZi3"
      },
      "source": [
        "## Installing torchtext used for pre-processing, loading, batching the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg2J3rZtESsW"
      },
      "source": [
        "!pip install torchtext==0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MA9X771Hp0d"
      },
      "source": [
        "## Installing indicnlp module for tokenizing hindi sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOnqfy0PE5OD"
      },
      "source": [
        "!git clone \"https://github.com/anoopkunchukuttan/indic_nlp_library\"\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install Morfessor\n",
        "INDIC_NLP_LIB_HOME=r\"/content/indic_nlp_library\"\n",
        "INDIC_NLP_RESOURCES=\"/content/indic_nlp_resources\"\n",
        "import sys\n",
        "sys.path.append(r'{}'.format(INDIC_NLP_LIB_HOME))\n",
        "from indicnlp import common\n",
        "common.set_resources_path(INDIC_NLP_RESOURCES)\n",
        "\n",
        "from indicnlp import loader\n",
        "loader.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mL4oItNBvSs"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import  re\n",
        "import string\n",
        "from unicodedata import normalize\n",
        "\n",
        "import torch\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "# Field used for pre-porcessing # TabularDataset to load data #BucketIterator used to construct iterator to do batching & padding\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator\n",
        "\n",
        "\n",
        "# For tokenizing english sentence\n",
        "import spacy\n",
        "# For tokenizing hindi sentence  \n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "# For train, test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU2g5gTVFhrd"
      },
      "source": [
        "## Setting up the device based on availabilty of GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adyBDgMY-mgm"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutXBe6Lv2I3"
      },
      "source": [
        "## Function that cleans Train data and saves the clean train data to a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM6mZvvSv1UO"
      },
      "source": [
        "def clean_train_data():\n",
        "  raw_data_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/train.csv',delimiter=',')\n",
        "  raw_data_df['hindi'].replace('', np.nan, inplace=True)\n",
        "  raw_data_df['english'].replace('', np.nan, inplace=True)\n",
        "  raw_data_df.dropna(subset=['hindi'], inplace=True)\n",
        "  raw_data_df.dropna(subset=['english'], inplace=True)\n",
        "\n",
        "  raw_data_df = raw_data_df.dropna(how = 'all')\n",
        "  raw_data_df.drop(raw_data_df[raw_data_df['hindi']==raw_data_df['english']].index,inplace = True)\n",
        "\n",
        "  raw_data_df.drop_duplicates(keep='first',inplace=True)\n",
        "\n",
        "  raw_data_df['hindi'] = raw_data_df['hindi'].str.strip()\n",
        "  raw_data_df['english'] = raw_data_df['english'].str.strip()\n",
        "  #removing lines for which translations are not relatively equal sized\n",
        "  raw_data_df['hindi_len'] = raw_data_df.apply(lambda row: len(row['hindi'].split()),axis=1)\n",
        "  raw_data_df['english_len'] = raw_data_df.apply(lambda row: len(row['english'].split()),axis=1)\n",
        "\n",
        "  raw_data_df.query('hindi_len < 80 & english_len < 80',inplace = True)\n",
        "  raw_data_df.query('hindi_len < english_len * 1.5 & english_len < 1.5*hindi_len',inplace = True)\n",
        "\n",
        "\n",
        "  ## normalizing, stripping punctuation,lowering english and hindi text ##\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for i in raw_data_df.index:\n",
        "      temp_str = raw_data_df['english'][i]\n",
        "      raw_data_df.at[i, \"english\"] = normalize('NFD',temp_str).encode('ascii', 'ignore')\n",
        "      raw_data_df.at[i, \"english\"] = raw_data_df.at[i, \"english\"].decode('UTF-8')\n",
        "      temp_str_words = raw_data_df.at[i, \"english\"].split()\n",
        "      temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "      #temp_str_words = [regx_printable.sub('', word) for word in temp_str_words]\n",
        "      temp_str_words = [word.lower() for word in temp_str_words]\n",
        "      temp_str_words = [word for word in temp_str_words if word.isalpha()]\n",
        "      raw_data_df.at[i, \"english\"] = ' '.join(map(str, temp_str_words))\n",
        "      \n",
        "      temp_str = raw_data_df['hindi'][i]\n",
        "      raw_data_df.at[i, \"hindi\"] = normalize('NFD',temp_str).encode('UTF-8', 'ignore')\n",
        "      raw_data_df.at[i, \"hindi\"] = raw_data_df.at[i, \"hindi\"].decode('UTF-8')\n",
        "      temp_str_words = raw_data_df.at[i, \"hindi\"].split()\n",
        "      temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "      #temp_str_words = [regx_printable.sub('', word) for word in temp_str_words]\n",
        "      temp_str_words = [word.lower() for word in temp_str_words]\n",
        "      raw_data_df.at[i, \"hindi\"] = ' '.join(map(str, temp_str_words))\n",
        "      #raw_data_df.at[i, \"hindi\"] = raw_data_df.at[i, \"hindi\"].replace(u\"\\u0964\".encode('UTF-8'), \"\")\n",
        "      #raw_data_df.at[i, \"hindi\"] = re.sub(r'\\|', '',  raw_data_df.at[i, \"hindi\"])\n",
        "\n",
        "  # numbers = str.maketrans('', '', string.digits)\n",
        "  # raw_data_df['english']=raw_data_df['english'].apply(lambda x: x.translate(numbers))\n",
        "  # raw_data_df['hindi']=raw_data_df['hindi'].apply(lambda x: x.translate(numbers))\n",
        "  # raw_data_df['hindi'] = raw_data_df['hindi'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "  # raw_data_df.drop(raw_data_df.columns[[0]], axis = 1,inplace=True)\n",
        "\n",
        "  raw_data_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv',index=False)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiBSEo5xzFBQ"
      },
      "source": [
        "## Function that cleans test data and saves it to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5oyL4EvzMpT"
      },
      "source": [
        "def clean_test_data():\n",
        "  raw_data_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/hindistatements.csv', delimiter=',')\n",
        "  raw_data_df['hindi'] = raw_data_df['hindi'].str.strip()\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  for i in raw_data_df.index:\n",
        "      temp_str = raw_data_df['hindi'][i]\n",
        "      raw_data_df.at[i, \"hindi\"] = normalize('NFD',temp_str).encode('UTF-8', 'ignore')\n",
        "      raw_data_df.at[i, \"hindi\"] = raw_data_df.at[i, \"hindi\"].decode('UTF-8')\n",
        "      temp_str_words = raw_data_df.at[i, \"hindi\"].split()\n",
        "      temp_str_words = [word.translate(table) for word in temp_str_words]\n",
        "      temp_str_words = [word.lower() for word in temp_str_words]\n",
        "      raw_data_df.at[i, \"hindi\"] = ' '.join(map(str, temp_str_words))\n",
        "    \n",
        "  raw_data_df.drop(raw_data_df.columns[[0,1]], axis = 1,inplace=True)\n",
        "  raw_data_df.at[4710,'hindi'] = '/ ?'\n",
        "  raw_data_df.to_csv('/content/drive/MyDrive/Colab_Notebooks/test_clean_data.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h17msSSAyzaO"
      },
      "source": [
        "## Cleaning the Training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZujINLkFyuv7"
      },
      "source": [
        "clean_train_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdo_MMw-y_00"
      },
      "source": [
        "## Cleaning the Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA3Vy7n-y-6f"
      },
      "source": [
        "clean_test_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yMSzH2oFtt4"
      },
      "source": [
        "## Load default spacy english-core-web model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcyNDKXDd4q"
      },
      "source": [
        "spacy_english = spacy.load('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RpOzneLGFxD"
      },
      "source": [
        "## Function that returns list of tokens in an english sentence using sacy tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hy1hVyxIABZl"
      },
      "source": [
        "def spacy_english_tokenizer(sentence):\n",
        "  tokenized_sent = spacy_english.tokenizer(sentence)\n",
        "  token_list = []\n",
        "  for token in tokenized_sent:\n",
        "    token_list.append(token.text)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00eMgbiTGE-I"
      },
      "source": [
        "## Function that returns list of tokens in a hindi sentence using indicnlp tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gspj8cvvFroQ"
      },
      "source": [
        "def indicnlp_hindi_tokenizer(sentence):\n",
        "  token_list = indic_tokenize.trivial_tokenize(sentence)\n",
        "  return token_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO9tggsrGVrG"
      },
      "source": [
        "## Testing english tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2iQlJKxyWA"
      },
      "source": [
        "sentence = 'hello world!'\n",
        "spacy_english_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40lLvDZkGbcR"
      },
      "source": [
        "## Testing hindi tokenizer on a sample sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUHY4pjQyk5A"
      },
      "source": [
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा।'\n",
        "indicnlp_hindi_tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww-pEfAyGmt3"
      },
      "source": [
        "### Defining Preprocessing pipelines for both hindi and english sentences using Field class in torchtext.data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBNOJYpBGSCs"
      },
      "source": [
        "english = Field(sequential=True ,init_token='sos', eos_token='eos', tokenize = spacy_english_tokenizer,is_target=True)\n",
        "hindi = Field(sequential=True ,init_token='sos',eos_token='eos',tokenize = indicnlp_hindi_tokenizer,stop_words=['।'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3lEs0wjIS-r"
      },
      "source": [
        "## Making a train, validation data split (20% data used for validation)\n",
        "## Saving train.csv, valid.csv to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfVYDimL_HJX"
      },
      "source": [
        "clean_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/clean_data.csv',delimiter=',')\n",
        "train, valid = train_test_split(clean_df, test_size = 0.2,random_state=123)\n",
        "train.to_csv('/content/drive/MyDrive/Colab_Notebooks/clean_train.csv',index=False)\n",
        "valid.to_csv('/content/drive/MyDrive/Colab_Notebooks/clean_valid.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWBzvHsYIjRh"
      },
      "source": [
        "## Loading the train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78N8oj8SAKgS"
      },
      "source": [
        "train_data, valid_data = TabularDataset.splits(\n",
        "        path='/content/drive/MyDrive/Colab_Notebooks/', train='clean_train.csv',\n",
        "        test='clean_valid.csv', format='csv',\n",
        "        fields={'hindi':('hindi',hindi),'english':('english',english)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh2R5hOQIsFi"
      },
      "source": [
        "## Building Hindi and English Vocabuary with maximum limit set to 18000 tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jLWG1Cejbsl"
      },
      "source": [
        "# building vocabulary of maximum size with 18000 tokens for both ennglish & hindi\n",
        "english.build_vocab(train_data,max_size=18000)\n",
        "hindi.build_vocab(train_data,max_size=18000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMUVSKLI7Xq"
      },
      "source": [
        "## Checking the indexes assigned to various tokens in english and hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXxO0C2_8BgO"
      },
      "source": [
        "english.vocab.stoi['<unk>']\n",
        "hindi.vocab.itos[10003]\n",
        "english.vocab.itos[7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNG6UsyBcD1u"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fieqzVHLygV"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,num_layers,dropout_p=0.1):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    self.embedding = nn.Embedding(input_size,embedding_size)\n",
        "    self.rnn = nn.LSTM(embedding_size,hidden_size,num_layers,dropout=dropout_p,bidirectional=True)\n",
        "\n",
        "  def forward(self,sample):\n",
        "    # print('sample'); print(sample[:,0])\n",
        "    embedding = self.embedding(sample)\n",
        "    embedding_withdropout = self.dropout(embedding)\n",
        "    encoder_outputs,(hidden,cell) = self.rnn(embedding_withdropout)\n",
        "    # print(hidden[0]); print(hidden[1])\n",
        "    # print(hidden.shape)\n",
        "    # print('hidden after');\n",
        "    hidden = 0.5*(hidden[0] + hidden[1])\n",
        "    cell = 0.5*(cell[0]+cell[1])\n",
        "    # print(hidden.shape); #print(cell.shape)\n",
        "    # hidden = 0.5 * torch.sum(hidden,dim=0)\n",
        "    hidden = hidden.unsqueeze(0)\n",
        "    cell = cell.unsqueeze(0)\n",
        "    # print(hidden); print(hidden.shape)\n",
        "    # print('encoder_hidden after mean:'); print(hidden)\n",
        "    return encoder_outputs,hidden,cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty3rLRJAcGSs"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cIkBH0bMtUG"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self,input_size,embedding_size,hidden_size,output_size,num_layers,dropout_p=0.1):\n",
        "    super(Decoder,self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.output_size = output_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    self.embedding_size= embedding_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.input_size,self.embedding_size)\n",
        "    self.rnn = nn.LSTM(self.embedding_size,hidden_size,num_layers,dropout=dropout_p)\n",
        "    self.fc = nn.Linear(self.hidden_size,self.output_size)\n",
        "\n",
        "  def forward(self,x,hidden,cell):\n",
        "    x = x.unsqueeze(0)\n",
        "    # print(x.shape)\n",
        "    embedding = self.embedding(x)\n",
        "    embedding_dropout = self.dropout(embedding)\n",
        "    # print(embedding.shape)\n",
        "    outputs, (hidden,cell) = self.rnn(embedding_dropout,(hidden,cell))\n",
        "    y_hats = self.fc(outputs)\n",
        "    # print('shape of y_hats before squeeze::')\n",
        "    # print(y_hats.shape)\n",
        "    y_hats = y_hats.squeeze(0)\n",
        "    # print('shape of y_hats after squeeze::')\n",
        "    # print(y_hats.shape)\n",
        "    # print(hidden.shape); print(cell.shape)\n",
        "    return y_hats,hidden,cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vKkU3ZNcKFQ"
      },
      "source": [
        "## Seq2Seq Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUNBCO3BMxln"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self,encoder,decoder):\n",
        "    super(Seq2Seq,self).__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "  \n",
        "  def forward(self,hindi_batch,english_batch,teacher_force_ratio = 0.5):\n",
        "    batch_size = hindi_batch.shape[1]\n",
        "    english_batch_len = english_batch.shape[0]\n",
        "    english_vocab_size = len(english.vocab)\n",
        "\n",
        "    outputs = torch.zeros(english_batch_len,batch_size,english_vocab_size).to(device)\n",
        "    encoder_outputs,hidden,cell = self.encoder(hindi_batch)\n",
        "\n",
        "    sample=english_batch[0]\n",
        "    # print('sample:'); print(sample)\n",
        "\n",
        "    for token in range(1,english_batch_len):\n",
        "      predict,hidden,cell = self.decoder(sample,hidden,cell)\n",
        "      # print(\"reached here\")\n",
        "      outputs[token] = predict\n",
        "      # print('outputs'); print(predict.shape); print(predict)\n",
        "      probable_token = predict.argmax(1)\n",
        "      # print('probable_token')\n",
        "      # print(probable_token)\n",
        "      sample = english_batch[token] if random.random() < teacher_force_ratio else probable_token\n",
        "      # print(english_batch[token].shape)\n",
        "    return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20zhiwtkJtjC"
      },
      "source": [
        "## Setting Seq to Seq Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5UB54mvJp7O"
      },
      "source": [
        "# input size to encoder\n",
        "hindi_one_hot_len = len(hindi.vocab)\n",
        "# input size and output size to decoder (both should be same as output of decoder is fed to decoder)\n",
        "english_one_hot_len = len(english.vocab)\n",
        "output_size = len(english.vocab)\n",
        "\n",
        "# hidden size\n",
        "hidden_size = 1024\n",
        "# number of epochs to be trained\n",
        "num_epochs = 60\n",
        "# num_epochs = 5\n",
        "# learning rate\n",
        "learning_rate = 0.001\n",
        "# batch size\n",
        "batch_size = 128\n",
        "#drop out rates for encoder & decoder\n",
        "dropout_p = 0.1\n",
        "\n",
        "# number of layers for encode & decoder\n",
        "num_layers = 1\n",
        "\n",
        "# encoder embedding size\n",
        "encoder_embedding_size = 300\n",
        "# decoder embedding size\n",
        "decoder_embedding_size = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHPu09n-LNtD"
      },
      "source": [
        "## Bucket iterator form trchtext that batches examples of similar lengths together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiprXnprLYQi"
      },
      "source": [
        "train_iterator,valid_iterator = BucketIterator.splits((train_data, valid_data),batch_size = batch_size,\n",
        "                                                     sort_within_batch=True, sort_key=lambda x: len(x.hindi),device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teYdxM15E30I"
      },
      "source": [
        "## Model Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPYsjnze_8wz"
      },
      "source": [
        "def init_weights(model):\n",
        "  for name, param in model.named_parameters():\n",
        "    nn.init.uniform_(param.data, -1, 1)\n",
        "    # nn.init.xavier_uniform_(param.data, gain=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNnrlJCwplfP"
      },
      "source": [
        "## Creating the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70aq4gJKcgox"
      },
      "source": [
        "encoder = Encoder(hindi_one_hot_len,encoder_embedding_size,\n",
        "                      hidden_size,num_layers,dropout_p).to(device)\n",
        "\n",
        "decoder = Decoder(english_one_hot_len,decoder_embedding_size,\n",
        "                      hidden_size,output_size,num_layers,dropout_p).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder,decoder).to(device)\n",
        "\n",
        "pad_index = english.vocab.stoi['<pad>']\n",
        "criterian = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
        "\n",
        "# init_weights(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m5GUJ1zMAtN"
      },
      "source": [
        "## Using Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7ooP-7MGni"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6Jr2hAKE7vY"
      },
      "source": [
        "## Clipping gradient function to avoid exploding gradient problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9iqPLp8E-4Q"
      },
      "source": [
        "def clip_gradients(model):\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s9cEteaaEKa"
      },
      "source": [
        "## Function to insert start and end tokens at the begining and end of a token list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COvBzI7cY2rQ"
      },
      "source": [
        "def insert_sos_eos(tokens):\n",
        "  tokens.insert(0, hindi.init_token)\n",
        "  tokens.append(hindi.eos_token)\n",
        "  return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYIn9P1OZ105"
      },
      "source": [
        "## Function that output's indices for a list of tokens from Hinidi Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayJlLXiiZWYf"
      },
      "source": [
        "def token_to_index(tokens):\n",
        "  indices = []\n",
        "  for token in tokens:\n",
        "    indices.append(hindi.vocab.stoi[token])\n",
        "  return indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E2yQkO7gFLO"
      },
      "source": [
        "## Function that output's tokens for a list of indices from English Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZypnML0ydm-t"
      },
      "source": [
        "def index_to_token(indices):\n",
        "  english_sentence = []\n",
        "  for index in indices:\n",
        "    english_sentence.append(english.vocab.itos[index])\n",
        "  return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NVCQtdWswdR"
      },
      "source": [
        "def hindi_to_english(model, text, hindi, english, device, max_length=15):\n",
        "    tokens = indicnlp_hindi_tokenizer(text)\n",
        "    tokens = insert_sos_eos(tokens)\n",
        "    indices_for_tokens = token_to_index(tokens)\n",
        "    ## unsqueeze the index list of input sentence to get a batch of single sentence\n",
        "    hindi_batch = torch.LongTensor(indices_for_tokens).unsqueeze(1).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        encoder_outputs,hidden, cell = model.encoder(hindi_batch)\n",
        "\n",
        "    decoder_output = [english.vocab.stoi[\"sos\"]]\n",
        "\n",
        "    for word in range(max_length):\n",
        "        last_inp_decoder = torch.LongTensor([decoder_output[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            current_output, hidden, cell = model.decoder(last_inp_decoder, hidden, cell)\n",
        "\n",
        "        decoder_output.append(current_output.argmax(1).item())\n",
        "        if current_output.argmax(1).item() == english.vocab.stoi[\"eos\"]:\n",
        "            break\n",
        "\n",
        "    english_sentence = index_to_token(decoder_output)\n",
        "    english_sentence.remove('sos')\n",
        "    if english_sentence[-1]=='eos':\n",
        "      english_sentence.pop()\n",
        "    return english_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY5nERqBcr_X"
      },
      "source": [
        "## Loading the saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNBAif8_ejUS"
      },
      "source": [
        "# model = torch.load('/content/drive/MyDrive/Colab_Notebooks/checkpoint_apr03_sub2.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuJ6UqCNczDr"
      },
      "source": [
        "## Training and Checking the loss on Validation data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilen4WLjetwq"
      },
      "source": [
        "training_loss = []\n",
        "validation_loss = []\n",
        "sentence = 'वहाँ पहुँचने में कितना समय लगेगा'\n",
        "min_loss = 10000\n",
        "for epoch in range(num_epochs):\n",
        "  batch_num = 1\n",
        "  torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_eachepoch_W2.pth')\n",
        "  model.train(True)\n",
        "  training_loss = []; validation_loss = []\n",
        "  for batch_index, batch in enumerate(train_iterator):\n",
        "    hindi_batch = batch.hindi.to(device)\n",
        "    english_batch = batch.english.to(device)\n",
        "\n",
        "    # print('inputdata:');print(hindi_batch.shape);print('outputdata:');print(english_batch.shape)\n",
        "    # print(hindi_batch[:,0]);print(hindi_batch[:,1]);print(hindi_batch[:,2]);print(hindi_batch[:,3]);print(hindi_batch[:,4])\n",
        "    # print(english_batch[:,0]);print(english_batch[:,1]);print(english_batch[:,2]);print(english_batch[:,3]);print(english_batch[:,4]);\n",
        "    output = model(hindi_batch,english_batch)\n",
        "\n",
        "    output = output[1:].reshape(-1,output.shape[2])\n",
        "    english_batch = english_batch[1:].reshape(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterian(output,english_batch)\n",
        "    training_loss.append(loss)\n",
        "    if loss.item() < min_loss:\n",
        "      min_loss = loss.item()\n",
        "      torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpoint_with_leastloss_W2.pth')\n",
        "    # print('loss'+str(loss))\n",
        "    # print('batch_num:'+str(batch_num))\n",
        "    loss.backward()\n",
        "    clip_gradients(model)\n",
        "    optimizer.step()\n",
        "    batch_num+=1\n",
        "\n",
        "  ######## running on validation set ########\n",
        "  with torch.no_grad():\n",
        "    model.eval()\n",
        "    for batch_index, batch in enumerate(valid_iterator):\n",
        "      hindi_batch = batch.hindi.to(device)\n",
        "      english_batch = batch.english.to(device)\n",
        "      output = model(hindi_batch,english_batch)\n",
        "      output = output[1:].reshape(-1,output.shape[2])\n",
        "      english_batch = english_batch[1:].reshape(-1)\n",
        "      loss = criterian(output,english_batch)\n",
        "      validation_loss.append(loss)\n",
        "  T_loss = sum(training_loss); V_loss = sum(validation_loss)\n",
        "  print(f'Epoch [{epoch+1}/{num_epochs}], training loss: {T_loss/len(train_iterator)}, validation_loss: {V_loss/len(valid_iterator)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VGnIS9qNz1x"
      },
      "source": [
        "# torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/afterepochs_apr03_sub2.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3PtuvuA0SJC"
      },
      "source": [
        "# FILE = 'checkpointV01.pth'\n",
        "# torch.save(model,'/content/drive/MyDrive/Colab_Notebooks/checkpointV01.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRQ3hf5ZTCb2"
      },
      "source": [
        "test_df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/test_clean_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzJkMCQwan6"
      },
      "source": [
        "# test_df.at[4994,'hindi']\n",
        "print(test_df.shape)\n",
        "f = open('/content/drive/MyDrive/Colab_Notebooks/answer.txt',\"w\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xpLR8VCeXmw"
      },
      "source": [
        "model.eval()\n",
        "for i in test_df.index:\n",
        "    sentence = str(test_df['hindi'][i])\n",
        "    # print(type(sentence))\n",
        "    english_sentence = hindi_to_english(model,sentence,hindi,english,device,max_length=40)\n",
        "    output_sen = ' '.join(map(str, english_sentence))\n",
        "    output_sen = output_sen + '\\n'\n",
        "    f.write(output_sen)\n",
        "    print(i)\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12cPE-oRjg6n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}